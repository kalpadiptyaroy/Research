{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Using PyTorch version 1.7.0+cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print('Using PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Covid', 'Non-covid']  # we have two classes covid and non-covid\n",
    "root_dir = 'archive1\\COVID-19 Dataset\\CT'\n",
    "source_dirs = ['Covid', 'Non-Covid']  # these are dirs where the labels images are stored. Names are co-incidentally same with that of class names.\n",
    "\n",
    "if os.path.isdir(os.path.join(root_dir, source_dirs[1])):\n",
    "    os.mkdir(os.path.join(root_dir, 'test'))\n",
    "\n",
    "    for i, d in enumerate(source_dirs):\n",
    "        os.rename(os.path.join(root_dir, d), os.path.join(root_dir, class_names[i]))\n",
    "\n",
    "    for c in class_names:\n",
    "        os.mkdir(os.path.join(root_dir, 'test', c))\n",
    "\n",
    "    for c in class_names:\n",
    "        images = [x for x in os.listdir(os.path.join(root_dir, c)) if (x[-3:].lower().endswith('png') or x[-3:].lower().endswith('jpg') or x[-4:].lower().endswith('jpeg'))]\n",
    "        selected_images = random.sample(images, 30)\n",
    "        for image in selected_images:\n",
    "            source_path = os.path.join(root_dir, c, image)\n",
    "            target_path = os.path.join(root_dir, 'test', c, image)\n",
    "            shutil.move(source_path, target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXRayDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_dirs, transform):\n",
    "        def get_images(class_name):\n",
    "            images = [x for x in os.listdir(image_dirs[class_name]) if (x[-3:].lower().endswith('png') or x[-3:].lower().endswith('jpg') or x[-4:].lower().endswith('jpeg')) ] \n",
    "            print(f'Found {len(images)} {class_name} examples')    \n",
    "            return images\n",
    "        \n",
    "        self.images = {}\n",
    "        self.class_names = ['Covid', 'Non-Covid']\n",
    "        \n",
    "        for c in self.class_names:\n",
    "            self.images[c] = get_images(c)\n",
    "            \n",
    "        self.image_dirs = image_dirs\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return sum([len(self.images[c]) for c in self.class_names])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        class_name = random.choice(self.class_names)\n",
    "        index = index % len(self.images[class_name])\n",
    "        image_name = self.images[class_name][index]\n",
    "        image_path = os.path.join(self.image_dirs[class_name], image_name)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        return self.transform(image), self.class_names.index(class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=(224,224)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(size=(224, 224)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 5367 Covid examples\nFound 2567 Non-Covid examples\n"
     ]
    }
   ],
   "source": [
    "train_dirs = {\n",
    "    'Covid': 'archive1/COVID-19 Dataset/CT/Covid',\n",
    "    'Non-Covid' : 'archive1/COVID-19 Dataset/CT/Non-Covid'\n",
    "}\n",
    "\n",
    "train_dataset = ChestXRayDataset(train_dirs, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Found 60 Covid examples\nFound 60 Non-Covid examples\n"
     ]
    }
   ],
   "source": [
    "test_dirs = {\n",
    "    'Covid': 'archive1/COVID-19 Dataset/CT/test/Covid',\n",
    "    'Non-Covid' : 'archive1/COVID-19 Dataset/CT/test/Non-Covid'\n",
    "}\n",
    "\n",
    "test_dataset = ChestXRayDataset(test_dirs, test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of Training batches 794\nNumber of test batches 12\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "\n",
    "dl_train = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "dl_test = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "print('Number of Training batches', len(dl_train))\n",
    "print('Number of test batches', len(dl_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "\n",
    "def show_images(images, labels, preds):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    for i, image in enumerate(images):\n",
    "        plt.subplot(1, 6, i + 1, xticks=[], yticks=[])\n",
    "        image = image.numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = image * std + mean\n",
    "        image = np.clip(image, 0., 1.)\n",
    "        plt.imshow(image)\n",
    "        col = 'green' \n",
    "        if preds[i] != labels[i]:\n",
    "            col = 'red'\n",
    "        plt.xlabel(f'{class_names[int(labels[i].numpy())]}')\n",
    "        plt.ylabel(f'{class_names[int(preds[i].numpy())]}', color=col)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dl_train))\n",
    "show_images(images, labels, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(dl_test))\n",
    "show_images(images, labels, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18.fc = torch.nn.Linear(in_features=512, out_features=2)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_preds():\n",
    "    resnet18.eval() #We set the resnet18 model to evaluation mode.\n",
    "    images, labels = next(iter(dl_test))\n",
    "    outputs = resnet18(images)\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    show_images(images, labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_preds"
   ]
  },
  {
   "source": [
    "# Checkpoints to Save the Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, accuracy_yaxis, val_loss_yaxis, steps_xaxis):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'accuracy': accuracy_yaxis,\n",
    "        'val_loss' : val_loss_yaxis,\n",
    "        'steps_xaxis': steps_xaxis\n",
    "    }\n",
    "    print(f'Progress Saved till epoch = {epoch}')\n",
    "    torch.save(checkpoint, 'E:\\\\Image Recognition\\\\covid-chestxray-dataset\\\\resnet18_checkpoint.pth')"
   ]
  },
  {
   "source": [
    "# Loading saved checkpoints"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer):\n",
    "    saved_checkpoint = torch.load('E:\\\\Image Recognition\\\\covid-chestxray-dataset\\\\resnet18_checkpoint.pth')\n",
    "    model.load_state_dict(saved_checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(saved_checkpoint['optimizer_state_dict'])\n",
    "    epoch = saved_checkpoint['epoch']\n",
    "    accuracy_yaxis = saved_checkpoint['accuracy']\n",
    "    val_loss_yaxis = saved_checkpoint['val_loss']\n",
    "    steps_xaxis = saved_checkpoint['steps_xaxis']\n",
    "    return epoch, accuracy_yaxis, val_loss_yaxis, steps_xaxis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, resume_from_checkpoint = False):\n",
    "    accuracy_yaxis = []\n",
    "    val_loss_yaxis = []\n",
    "    train_loss_yaxis = []\n",
    "    train_accuracy_yaxis = []\n",
    "    steps_xaxis = []\n",
    "\n",
    "    if resume_from_checkpoint == True:\n",
    "        starting_epoch, accuracy_yaxis, val_loss_yaxis, steps_xaxis = load_checkpoint(resnet18, optimizer)\n",
    "        print('Starting Epoch = ', starting_epoch)\n",
    "        print('Resumed from Last Saved Checkpoint')\n",
    "    else:\n",
    "        starting_epoch = 0\n",
    "\n",
    "    print('Starting Training ...')\n",
    "    for e in range(starting_epoch, starting_epoch + epochs):\n",
    "        print('_'*20)\n",
    "        print(f'Starting epoch {e + 1} / {starting_epoch + epochs}')\n",
    "        print('_'*20)\n",
    "\n",
    "        train_loss, val_loss = 0, 0\n",
    "\n",
    "        resnet18.train() #we put the resnet model to train mode.\n",
    "\n",
    "        # -----------------  Training Starts here. --------------------------\n",
    "\n",
    "        for train_step, (images, labels) in enumerate(dl_train):\n",
    "            optimizer.zero_grad() #Before training our gradient is always zero.\n",
    "            outputs = resnet18(images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if train_step % 20 == 0:\n",
    "                print('Evaluating at step', train_step)\n",
    "                _, p = torch.max(outputs, 1)\n",
    "                accuracy = sum((p == labels).numpy()) / len(train_dataset)\n",
    "                train_accuracy_yaxis.append(accuracy)\n",
    "                accuracy = 0\n",
    "                resnet18.eval() #We put the resnet18 model to evaluation mode.\n",
    "\n",
    "                for val_step, (images, labels) in enumerate(dl_test):\n",
    "                    outputs = resnet18(images)\n",
    "                    loss = loss_fn(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    accuracy += sum((preds == labels).numpy()) #vector form or array form example [0, 1, 1, ....., 0, 1]\n",
    "                    \n",
    "                #val_accuracy_yaxis.append(accuracy)\n",
    "                val_loss /= (val_step + 1)\n",
    "                accuracy = accuracy / len(test_dataset)\n",
    "                print(f'Validation Loss: {val_loss:.4f}, Accuracy:{accuracy:.4f}')\n",
    "\n",
    "                accuracy_yaxis.append(accuracy)\n",
    "                val_loss_yaxis.append(val_loss)\n",
    "                steps_xaxis.append(((len(dl_train) - 1) * e) + train_step)\n",
    "                #show_preds()\n",
    "                resnet18.train()\n",
    "\n",
    "                if accuracy >= 0.99:\n",
    "                    print('Performance Condition Satisfied, Stopping ...')\n",
    "                    save_checkpoint(resnet18, optimizer, e + 1, accuracy_yaxis, val_loss_yaxis, steps_xaxis)\n",
    "                    return (accuracy_yaxis, val_loss_yaxis, train_loss_yaxis, train_accuracy_yaxis, steps_xaxis)\n",
    "\n",
    "        train_loss /= (train_step + 1)\n",
    "        train_loss_yaxis.append(train_loss)\n",
    "\n",
    "        print(f'Training Loss: {train_loss:.4f}')\n",
    "\n",
    "        if (e + 1) % 5 == 0:\n",
    "            save_checkpoint(resnet18, optimizer, e + 1, accuracy_yaxis, val_loss_yaxis, steps_xaxis)\n",
    "    \n",
    "    print('Training Complete ....')\n",
    "    return (accuracy_yaxis, val_loss_yaxis, train_loss_yaxis, train_accuracy_yaxis, steps_xaxis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting Training ...\n",
      "____________________\n",
      "Starting epoch 1 / 2\n",
      "____________________\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.8443, Accuracy:0.4917\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.5111, Accuracy:0.8333\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.3972, Accuracy:0.8417\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.3438, Accuracy:0.8417\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.3183, Accuracy:0.8833\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.2831, Accuracy:0.8750\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.3153, Accuracy:0.8833\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.2868, Accuracy:0.8917\n",
      "Evaluating at step 160\n",
      "Validation Loss: 0.2866, Accuracy:0.8917\n",
      "Evaluating at step 180\n",
      "Validation Loss: 0.3176, Accuracy:0.8750\n",
      "Evaluating at step 200\n",
      "Validation Loss: 0.3639, Accuracy:0.8417\n",
      "Evaluating at step 220\n",
      "Validation Loss: 0.2801, Accuracy:0.8583\n",
      "Evaluating at step 240\n",
      "Validation Loss: 0.2457, Accuracy:0.8833\n",
      "Evaluating at step 260\n",
      "Validation Loss: 0.2872, Accuracy:0.8667\n",
      "Evaluating at step 280\n",
      "Validation Loss: 0.2996, Accuracy:0.8667\n",
      "Evaluating at step 300\n",
      "Validation Loss: 0.3126, Accuracy:0.8500\n",
      "Evaluating at step 320\n",
      "Validation Loss: 0.3359, Accuracy:0.8750\n",
      "Evaluating at step 340\n",
      "Validation Loss: 0.3844, Accuracy:0.9000\n",
      "Evaluating at step 360\n",
      "Validation Loss: 0.5090, Accuracy:0.8583\n",
      "Evaluating at step 380\n",
      "Validation Loss: 0.3672, Accuracy:0.8750\n",
      "Evaluating at step 400\n",
      "Validation Loss: 0.2661, Accuracy:0.8917\n",
      "Evaluating at step 420\n",
      "Validation Loss: 0.3120, Accuracy:0.9083\n",
      "Evaluating at step 440\n",
      "Validation Loss: 0.3518, Accuracy:0.8667\n",
      "Evaluating at step 460\n",
      "Validation Loss: 0.2060, Accuracy:0.9250\n",
      "Evaluating at step 480\n",
      "Validation Loss: 0.2746, Accuracy:0.9000\n",
      "Evaluating at step 500\n",
      "Validation Loss: 0.3063, Accuracy:0.8833\n",
      "Evaluating at step 520\n",
      "Validation Loss: 0.1583, Accuracy:0.9250\n",
      "Evaluating at step 540\n",
      "Validation Loss: 0.2218, Accuracy:0.9250\n",
      "Evaluating at step 560\n",
      "Validation Loss: 0.2123, Accuracy:0.9167\n",
      "Evaluating at step 580\n",
      "Validation Loss: 0.1190, Accuracy:0.9583\n",
      "Evaluating at step 600\n",
      "Validation Loss: 0.2194, Accuracy:0.9167\n",
      "Evaluating at step 620\n",
      "Validation Loss: 0.1630, Accuracy:0.9500\n",
      "Evaluating at step 640\n",
      "Validation Loss: 0.1847, Accuracy:0.9417\n",
      "Evaluating at step 660\n",
      "Validation Loss: 0.2687, Accuracy:0.8833\n",
      "Evaluating at step 680\n",
      "Validation Loss: 0.1736, Accuracy:0.9167\n",
      "Evaluating at step 700\n",
      "Validation Loss: 0.1752, Accuracy:0.9333\n",
      "Evaluating at step 720\n",
      "Validation Loss: 0.2217, Accuracy:0.9333\n",
      "Evaluating at step 740\n",
      "Validation Loss: 0.2292, Accuracy:0.9167\n",
      "Evaluating at step 760\n",
      "Validation Loss: 0.2288, Accuracy:0.9083\n",
      "Evaluating at step 780\n",
      "Validation Loss: 0.2109, Accuracy:0.9250\n",
      "Training Loss: 0.1983\n",
      "____________________\n",
      "Starting epoch 2 / 2\n",
      "____________________\n",
      "Evaluating at step 0\n",
      "Validation Loss: 0.1768, Accuracy:0.9250\n",
      "Evaluating at step 20\n",
      "Validation Loss: 0.2477, Accuracy:0.8667\n",
      "Evaluating at step 40\n",
      "Validation Loss: 0.1778, Accuracy:0.9167\n",
      "Evaluating at step 60\n",
      "Validation Loss: 0.1960, Accuracy:0.9250\n",
      "Evaluating at step 80\n",
      "Validation Loss: 0.1910, Accuracy:0.9417\n",
      "Evaluating at step 100\n",
      "Validation Loss: 0.2298, Accuracy:0.9333\n",
      "Evaluating at step 120\n",
      "Validation Loss: 0.2113, Accuracy:0.9417\n",
      "Evaluating at step 140\n",
      "Validation Loss: 0.2929, Accuracy:0.9250\n",
      "Evaluating at step 160\n",
      "Validation Loss: 0.2269, Accuracy:0.9083\n",
      "Evaluating at step 180\n",
      "Validation Loss: 0.2053, Accuracy:0.9250\n",
      "Evaluating at step 200\n",
      "Validation Loss: 0.2366, Accuracy:0.9083\n",
      "Evaluating at step 220\n",
      "Validation Loss: 0.2859, Accuracy:0.9083\n",
      "Evaluating at step 240\n",
      "Validation Loss: 0.1710, Accuracy:0.9583\n",
      "Evaluating at step 260\n",
      "Validation Loss: 0.2291, Accuracy:0.8917\n",
      "Evaluating at step 280\n",
      "Validation Loss: 0.1984, Accuracy:0.9167\n",
      "Evaluating at step 300\n",
      "Validation Loss: 0.2131, Accuracy:0.9167\n",
      "Evaluating at step 320\n",
      "Validation Loss: 0.3048, Accuracy:0.9167\n",
      "Evaluating at step 340\n",
      "Validation Loss: 0.2262, Accuracy:0.9083\n",
      "Evaluating at step 360\n",
      "Validation Loss: 0.2122, Accuracy:0.9083\n",
      "Evaluating at step 380\n",
      "Validation Loss: 0.3703, Accuracy:0.9083\n",
      "Evaluating at step 400\n",
      "Validation Loss: 0.1636, Accuracy:0.9333\n",
      "Evaluating at step 420\n",
      "Validation Loss: 0.2939, Accuracy:0.8750\n",
      "Evaluating at step 440\n",
      "Validation Loss: 0.2588, Accuracy:0.9250\n",
      "Evaluating at step 460\n",
      "Validation Loss: 0.2435, Accuracy:0.9250\n",
      "Evaluating at step 480\n",
      "Validation Loss: 0.1937, Accuracy:0.9417\n",
      "Evaluating at step 500\n",
      "Validation Loss: 0.2648, Accuracy:0.9250\n",
      "Evaluating at step 520\n",
      "Validation Loss: 0.2686, Accuracy:0.9083\n",
      "Evaluating at step 540\n",
      "Validation Loss: 0.2939, Accuracy:0.9000\n",
      "Evaluating at step 560\n",
      "Validation Loss: 0.2235, Accuracy:0.9167\n",
      "Evaluating at step 580\n",
      "Validation Loss: 0.2482, Accuracy:0.9417\n",
      "Evaluating at step 600\n",
      "Validation Loss: 0.2304, Accuracy:0.9417\n",
      "Evaluating at step 620\n",
      "Validation Loss: 0.2195, Accuracy:0.9000\n",
      "Evaluating at step 640\n",
      "Validation Loss: 0.1866, Accuracy:0.9333\n",
      "Evaluating at step 660\n",
      "Validation Loss: 0.2117, Accuracy:0.9417\n",
      "Evaluating at step 680\n",
      "Validation Loss: 0.1731, Accuracy:0.9250\n",
      "Evaluating at step 700\n",
      "Validation Loss: 0.1217, Accuracy:0.9667\n",
      "Evaluating at step 720\n",
      "Validation Loss: 0.1646, Accuracy:0.9250\n",
      "Evaluating at step 740\n",
      "Validation Loss: 0.1174, Accuracy:0.9667\n",
      "Evaluating at step 760\n",
      "Validation Loss: 0.1682, Accuracy:0.9583\n",
      "Evaluating at step 780\n",
      "Validation Loss: 0.3033, Accuracy:0.8917\n",
      "Training Loss: 0.0957\n",
      "Training Complete ....\n",
      "Wall time: 1h 23min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "accuracy_yaxis, val_loss_yaxis, train_loss_yaxis, train_accuracy_yaxis, steps_xaxis = train(epochs = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_preds()"
   ]
  },
  {
   "source": [
    "# Plotting the Results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'steps_xaxis' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7b43295aab12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ggplot\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_xaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss_yaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'validation loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msteps_xaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_yaxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'validation accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#plt.plot(steps_xaxis, train_loss_yaxis, label = 'train loss')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'steps_xaxis' is not defined"
     ]
    }
   ],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(steps_xaxis, val_loss_yaxis, label = 'validation loss')\n",
    "plt.plot(steps_xaxis, accuracy_yaxis, label = 'validation accuracy')\n",
    "#plt.plot(steps_xaxis, train_loss_yaxis, label = 'train loss')\n",
    "#plt.plot(steps_xaxis, train_accuracy_yaxis, label = 'train accuracy')\n",
    "plt.legend()\n",
    "plt.title('(Validation Loss , Validation Accuracy) VS Train Step - Resnet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5bd12373e5d6177196c9c78a48a80ff66efba26779122770cab5c0b555a2ad2c"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}